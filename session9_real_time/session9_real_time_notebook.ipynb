{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 9: Running Deep Generative AI models in Real-Time \n",
    "\n",
    "Agenda:\n",
    "- Introduction: PyTorch and TorchScript\n",
    "- 8-bit linear quantization\n",
    "- ONNX and Graph Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: PyTorch and TorchScript\n",
    "\n",
    "<center><img src=\"./assets/torchscript.png\" width=\"50%\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example:**\n",
    "\n",
    "The `@torch.jit.script` decorator tells PyTorch to compile the function to\n",
    "TorchScript, capturing its control flow and making it runnable in the C++\n",
    "runtime.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "@torch.jit.script\n",
    "def weighted_sum(tensors: List[torch.Tensor], weights: List[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute a weighted sum over a list of tensors.\n",
    "    \n",
    "    Args:\n",
    "        tensors (List[torch.Tensor]):   N tensors of identical shape.\n",
    "        weights (List[float]):          N scalar weights (same length as `tensors`).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor:   Î£_i  weights[i] * tensors[i]\n",
    "    \"\"\"\n",
    "\n",
    "    # Basic runtime check (TorchScript preserves the assert)\n",
    "    assert len(tensors) == len(weights), \"Tensors and weights must be the same length.\"\n",
    "\n",
    "    # The loop and control flow are traced by TorchScript\n",
    "    out: torch.Tensor = torch.zeros_like(tensors[0])\n",
    "    for t, w in zip(tensors, weights):\n",
    "        out = out + t * w\n",
    "\n",
    "    return out\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** TorchScript is _statically typed_, meaning that all values should have a\n",
    "_monomorphic type_. These types should be specified using the `typing` module or\n",
    "any other module that allows static type checking (e.g. MyPy).\n",
    "\n",
    "</div>\n",
    "\n",
    "The function can then be compiled and saved to a `.pt` file with:\n",
    "\n",
    "```python\n",
    "torch.jit.save(weighted_sum, \"weighted_sum.pt\")\n",
    "```\n",
    "\n",
    "Finally, the compiled TorchScript module can be run in C++ using the `libtorch`\n",
    "API.\n",
    "\n",
    "```cpp\n",
    "#include <torch/script.h>\n",
    "\n",
    "int main() {\n",
    "    torch::jit::script::Module module = torch::jit::load(\"weighted_sum.pt\");\n",
    "\n",
    "    // Create weights\n",
    "    at::Tensor a = torch::tensor({1.0, 2.0, 3.0});\n",
    "    at::Tensor b = torch::tensor({10.0, 20.0, 30.0});\n",
    "    std::vector<at::Tensor> tensors = {a, b};\n",
    "\n",
    "    // Create weights\n",
    "    std::vector<double> weights = {0.3, 0.7};\n",
    "\n",
    "    // Pack arguments into a list of IValue\n",
    "    std::vector<torch::IValue> inputs;\n",
    "    inputs.push_back(tensors);\n",
    "    inputs.push_back(weights);\n",
    "\n",
    "    // Run the function\n",
    "    at::Tensor output = module.forward(inputs).toTensor();\n",
    "\n",
    "    // Print the output\n",
    "    std::cout << \"Result: \" << output << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "The execution time of the C++ program should be significantly faster than the\n",
    "Python one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Linear Quantization\n",
    "_Inspired by [https://huggingface.co/docs/optimum/en/concept_guides/quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)._\n",
    "\n",
    "![](./assets/float32_int8.png)\n",
    "\n",
    "We want to lower the bit width of our weights and activations from 32-bit\n",
    "floating point numbers (`float32`) to 8-bit integers (`int8`). This effectively\n",
    "decreases the model size and allows faster inference. This is useful for\n",
    "real-time use, or deployment on edge devices. Some libraries have been\n",
    "introduced to perform this kind of quantization, such as\n",
    "[BitsAndByte](https://github.com/bitsandbytes-foundation/bitsandbytes) or \n",
    "[ðŸ¤— optimum](https://github.com/huggingface/optimum).\n",
    "\n",
    "By performing 8-bit quantization, we are representing different `float32` values\n",
    "as `int8` values (which are comprised between 0-256). To do so, we create a\n",
    "linear projection from the `float32` space to the `int8` space through the\n",
    "formula:\n",
    "\n",
    "$$\n",
    "x = S \\cdot (x_q - Z)\n",
    "$$\n",
    "\n",
    "where $x$ is the `float32` value, $x_q$ is the quantized `int8` value, and $S$\n",
    "and $Z$ are the quantization parameters. Namely:\n",
    "- $S$ is the scale (a positive `float32`)\n",
    "- $Z$ is the zero-point, which is the `int8` value corresponding to 0 in the\n",
    "`float32` space.\n",
    "\n",
    "To compute these quantization parameters, we need to know the range of `float32`\n",
    "values that we can deal with. While this is known when quantizing weights, we\n",
    "do not have access to the range of values that our model can encounter when\n",
    "quantizing activation values. To estimate a range, we have different strategies:\n",
    "\n",
    "- (Post-Training) **Dyanmic Quantization**: The range for each activation is\n",
    "calculated at _runtime_. This increases the cost of inference, but generally\n",
    "achieves a higher accuracy.\n",
    "\n",
    "- (Post-Training) **Static Quantization**: The range for each activation is\n",
    "determined at _quantization time_ using a set of inputs called _calibration data_.\n",
    "\n",
    "- **Quantization-Aware Training**: The range for each activation is calculated\n",
    "at _training time_, by estimating the error introduced by quantization during\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX and Graph Optimizations\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is an open standard for representing Machine\n",
    "Learning models. It defines a common **set of operators** (_opset_) and a \n",
    "common **file format** (`.onnx`) to facilitate _interoperability_ between\n",
    "different deep learning frameworks.\n",
    "\n",
    "<center><img src=\"assets/onnx.png\" /></center>\n",
    "\n",
    "Image from [https://microsoft.github.io/ai-at-edge/docs/onnx/](https://microsoft.github.io/ai-at-edge/docs/onnx/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the ONNX model for `music-medium-800k`\n",
    "\n",
    "We use [netron.app](https:?/netron.app) to create the visualization of the\n",
    "model.\n",
    "\n",
    "![](./assets/netron1.png)\n",
    "![](./assets/netron2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Optimizations\n",
    "\n",
    "**[ONNX Runtime](https://onnxruntime.ai)** is a library that provides tools for the\n",
    "optimization and acceleration of inferencing of ONNX models. It supports a wide\n",
    "range of hardware backends (CUDA, DirectML, CoreML, etc.) and offers APIs for\n",
    "several programming languages.\n",
    "\n",
    "On top of allowing **8-bit linear quantization**, ONNX Runtime allows different\n",
    "kinds of ONNX model [graph optimizations](https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html):\n",
    "\n",
    "- _Constant Folding_ removes constant nodes from the graph by statically\n",
    "computing their output.\n",
    "\n",
    "- _Redundant Node Eliminations_ removes redundant nodes such as _identity_,\n",
    "_slice_, _unsqueeze_, or _dropout_.\n",
    "\n",
    "- _Node Fusions_ folds multiples nodes into a single node to improve efficiency.\n",
    "For example, a _matrix multiplication_ followed by an _addition_ can be merged\n",
    "into a more efficient **Gemm** (General Matrix Multiplication) node.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: center; margin: 0 auto; width: fit-content;\">\n",
    "  <img src=\"assets/graph_fusion_matmul_add.png\" style=\"padding: 2em\" width=\"50%\">\n",
    "  <span style=\"width:100%\">-></span>\n",
    "  <img src=\"assets/graph_fusion_gemm.png\" style=\"padding: 2em\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Other Machine Learning formats and standards\n",
    "\n",
    "Lots: _GGML_, _CoreML_, _Apache TVM_, _LLVM MLIR_, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
