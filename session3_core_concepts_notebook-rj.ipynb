{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Core Machine Learning Concepts for Music and Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda:**\n",
    "\n",
    "- Statistical Basics of Generative Modeling in Artificial Intelligence\n",
    "- Variational Autoencoders, expressing loss\n",
    "- **Hands On:**\n",
    "    1. Studying datasets: Lakh MIDI and Free Music Archive\n",
    "    2. Use and train RAVE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Basics of Generative Modeling in Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/discriminative_vs_generative.png)\n",
    "\n",
    "-> In a **Discriminative task**, we want to learn how to go from a *sample* $x$\n",
    "to a *label* $y$. There is one ideal output that we want to model.\n",
    "\n",
    "-> In a **Generative task**, we want to learn how to go from a *label* $y$ to a\n",
    "*sample* $x$. There are many different possible outputs. **This involves\n",
    "modeling a distribution properly to generate *new samples***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative AI is Distribution Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/data_distribution_estimation.png)\n",
    "\n",
    "-> In our estimated data distribution, high probability connects to data that is\n",
    "*likely* to be found in the initial distribution, while low probability connects\n",
    "to data that is *unlikely* to be found in the initial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Sampling\n",
    "\n",
    "![](./assets/distribution_sampling.png)\n",
    "\n",
    "-> Generative AI allows us to map from a *simple distribution* to a more\n",
    "*complex* data distribution. By sampling from our simple distribution and going\n",
    "through a series of operations, we can obtain data that matches our target\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variatial Autoencoders\n",
    "\n",
    "In Variational autoencoders, our *simple distribution* $\\pi$ is a latent space.\n",
    "\n",
    "![](./assets/vae.png)\n",
    "\n",
    "Our **generator** is parameterized by a set of weights $\\theta$, giving us\n",
    "$p_\\theta(x|z)$. Our output data distribution is $p_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: $p_{data}(x) \\approx p_\\theta(x)$\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "We'll be going over some maths that will be helpful once we start looking at\n",
    "diffusion models.\n",
    "</div>\n",
    "\n",
    "<center><img src=\"./assets/pdata_ptheta.png\" width=\"50%\" /></center>\n",
    "\n",
    "To approximate $p_{data}(x)$, we try to **minimize the *Kullback-Leibler* (KL)\n",
    "divergence**:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathcal{D}_\\text{KL} (p_{data} \\; || \\; p_\\theta)\n",
    "$$\n",
    "\n",
    "With some maths, we can prove that this is equivalent to **maximizing the\n",
    "likelihood of our target distribution**.\n",
    "\n",
    "\\begin{align*}\n",
    "&\\phantom{=} \\arg \\min_\\theta \\mathcal{D}_\\text{KL} (p_{data} \\; || \\; p_\\theta) \\\\\n",
    "&= \\arg \\min_\\theta \\sum_x p_{data}(x) \\log \\frac{p_{data}(x)}{p_\\theta(x)} \\\\\n",
    "&= \\arg \\min_\\theta \\sum_x -p_{data}(x) \\log p_\\theta(x) + const \\\\\n",
    "&= \\arg \\max_\\theta \\sum_x p_{data}(x) \\log p_\\theta(x) \\\\\n",
    "&= \\arg \\max_\\theta \\mathop{\\mathbb{E}}_{x \\sim p_{data}} \\log p_\\theta(x)\n",
    "\\end{align*}\n",
    "\n",
    "Note: $p_\\theta(x)$ is defined as:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\int_z p_\\theta(x|z)p(z)dz\n",
    "$$\n",
    "\n",
    "But we cannot control the \"true\" distribution $p(z)$. So we introduce a\n",
    "controllable distribution $q_\\phi(z|x)$. This allows us (through some maths shown here)\n",
    "to calculate a **lower bound** of $\\log p_\\theta(x)$ called the ELBO (Evidence\n",
    "Lower BOund).\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x) \\geq \\mathop{\\mathbb{E}}_{z \\sim q_\\phi(z|x)} \\big[ \\log p_\\theta (x | z) \\big] - \\mathcal{D}_\\text{KL}\\big(q_\\phi(z|x) \\; || \\; p_\\theta(z) \\big)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note:\n",
    "- The first term is the **reconstruction loss**, which quantifies *how well* the\n",
    "decoder can reconstruct $x$ from a sampled $z$.\n",
    "- The second term is the **regularization loss**, which forces the learned\n",
    "distribution $q_\\phi(z|x)$ to be closed to a prior $p(z)$ (e.g., a Gaussian).\n",
    "\n",
    "</div>\n",
    "\n",
    "Now, our model relies on us sampling $z \\sim q_\\phi(z|x)$, but we cannot\n",
    "**backpropagate** through a sampling operation! As a result, we rewrite our\n",
    "latent space as:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon, \\; \\;\\epsilon \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "<center><img src=\"./assets/vae_final.png\"/></center>\n",
    "\n",
    "Our final loss function is an objective to **minimize**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\theta,\\phi} = {\\color{orange} \\mathop{\\mathbb{E}}_{x \\sim p_{data}(x)} \\big[} - \\mathop{\\mathbb{E}}_{z \\sim q_\\phi(z|x)[\\log p_\\theta(x|z)] + \\mathcal{D}_\\text{KL}(q_\\phi(z|x) \\; || \\; p(z))} {\\color{orange} \\big]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 1: Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: Lakh MIDI Dataset (LMD)\n",
    "\n",
    "Available at https://colinraffel.com/projects/lmd/. We will use the\n",
    "\"lmd-matched\" subset, which aligns tracks to records (with metadata) from the\n",
    "Million Song Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the dataset directory if it doesn't exist\n",
    "!mkdir -p ../datasets\n",
    "\n",
    "# Download the dataset and metadata\n",
    "!wget -O ../datasets/lmd_matched.tar.gz http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz\n",
    "!wget -O ../datasets/msd_summary_file.h5 http://millionsongdataset.com/sites/default/files/AdditionalFiles/msd_summary_file.h5\n",
    "\n",
    "# Unzip the dataset\n",
    "!tar -xvzf ../datasets/lmd_matched.tar.gz -C ../datasets\n",
    "\n",
    "# Remove the tar.gz file after extraction\n",
    "!rm ../datasets/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file and read the metadata\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(\"../datasets/msd_summary_file.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the metadata into a pandas DataFrame\n",
    "analysis_df = pd.DataFrame(f[\"analysis\"][\"songs\"][:])\n",
    "metadata_df = pd.DataFrame(f[\"metadata\"][\"songs\"][:])\n",
    "\n",
    "# Concatenate the two DataFrames (they should be ordered the same)\n",
    "lmd_df = pd.concat([analysis_df, metadata_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Million Song Dataset contains more tracks than LMD-matched, so we will\n",
    "# need to filter some out\n",
    "\n",
    "# First, let's find all of the tracks that are in the LMD-matched dataset\n",
    "# (the track_ids are the names of the directories)\n",
    "import os\n",
    "\n",
    "lmd_matched_dir = \"../datasets/lmd_matched\"\n",
    "track_ids = set()\n",
    "for root, dirs, files in os.walk(lmd_matched_dir):\n",
    "    # Only add the track name if it is a final directory (with files)\n",
    "    if len(files) > 0:\n",
    "        track_ids.add(os.path.basename(root))\n",
    "\n",
    "# Print the number of tracks in the LMD-matched dataset\n",
    "print(f\"Number of tracks in LMD-matched dataset: {len(track_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter the DataFrame to only include the tracks that are in\n",
    "# the LMD-matched dataset\n",
    "lmd_df = lmd_df[lmd_df[\"track_id\"].astype(str).isin(track_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the columns of the DataFrame to see what metadata is available\n",
    "lmd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print how many different artists and albums we have\n",
    "print(\"Number of artists:\", len(lmd_df[\"artist_name\"].unique()))\n",
    "print(\"Number of albums:\", len(lmd_df[\"release\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We can use matplotlib to print some data distributions\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "# Plot the distribution of durations\n",
    "ax[0].hist(lmd_df[\"duration\"], bins=100)\n",
    "ax[0].set_title(\"Duration distribution\")\n",
    "ax[0].set_xlabel(\"Duration (seconds)\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot the distribution of tempos\n",
    "ax[1].hist(lmd_df[\"tempo\"], bins=100)\n",
    "ax[1].set_title(\"Tempo distribution\")\n",
    "ax[1].set_xlabel(\"Tempo (BPM)\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot the distribution of keys\n",
    "ax[2].hist(lmd_df[\"key\"], bins=12)\n",
    "ax[2].set_title(\"Key distribution\")\n",
    "ax[2].set_xlabel(\"Key\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "ax[2].set_xticks(range(12), [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"F#\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"])\n",
    "\n",
    "# Plot the distribution of loudness\n",
    "ax[3].hist(lmd_df[\"loudness\"], bins=100)\n",
    "ax[3].set_title(\"Loudness distribution\")\n",
    "ax[3].set_xlabel(\"Loudness\")\n",
    "ax[3].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to a song from the dataset\n",
    "\n",
    "import midi2audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi2audio_obj = midi2audio.FluidSynth(\"../session2_setup/assets/soundfont.sf2\")\n",
    "midi2audio_obj.midi_to_audio(\"../datasets/lmd_matched/G/D/V/TRGDVGJ128F92FFA60/0b75fc85e0d028c29350a0ee9c148ed1.mid\", \"assets/lmd_example.wav\")\n",
    "\n",
    "\n",
    "display(Audio(\"assets/lmd_example.wav\", rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Free Music Archive\n",
    "\n",
    "Available at https://github.com/mdeff/fma. We will use the \"fma_small\" subset,\n",
    "but feel free to explore other, larger subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the dataset directory if it doesn't exist\n",
    "!mkdir -p ../datasets\n",
    "\n",
    "# Download the dataset and metadata\n",
    "!wget -O ../datasets/fma_small.zip https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
    "!wget -O ../datasets/fma_metadata.zip https://os.unil.cloud.switch.ch/fma/fma_metadata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to unzip the dataset in Python (unzip doesn't support PKZIP)\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(\"../datasets/fma_small.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../datasets\")\n",
    "\n",
    "# Unzip the metadata\n",
    "with zipfile.ZipFile(\"../datasets/fma_metadata.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../datasets\")\n",
    "\n",
    "# Remove the zip files after extraction\n",
    "os.remove(\"../datasets/fma_small.zip\")\n",
    "os.remove(\"../datasets/fma_metadata.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load CSV metadata for FMA \n",
    "fma_df = pd.read_csv(\"../datasets/fma_metadata/tracks.csv\", index_col=0, header=[0, 1])\n",
    "\n",
    "# Some logic from FMA's `utils.py`\n",
    "COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
    "            ('track', 'genres'), ('track', 'genres_all')]\n",
    "for column in COLUMNS:\n",
    "    fma_df[column] = fma_df[column].map(ast.literal_eval)\n",
    "\n",
    "COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
    "            ('album', 'date_created'), ('album', 'date_released'),\n",
    "            ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
    "            ('artist', 'active_year_end')]\n",
    "for column in COLUMNS:\n",
    "    fma_df[column] = pd.to_datetime(fma_df[column])\n",
    "\n",
    "SUBSETS = ('small', 'medium', 'large')\n",
    "fma_df['set', 'subset'] = fma_df['set', 'subset'].astype(\n",
    "    pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
    "\n",
    "COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
    "            ('album', 'type'), ('album', 'information'),\n",
    "            ('artist', 'bio')]\n",
    "for column in COLUMNS:\n",
    "    fma_df[column] = fma_df[column].astype('category')\n",
    "\n",
    "# Filter the DataFrame to only include the tracks that are in the FMA small dataset\n",
    "fma_df = fma_df[fma_df[\"set\"][\"subset\"] == \"small\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the number of tracks, artists, and albums\n",
    "print(\"Number of tracks:\", len(fma_df))\n",
    "print(\"Number of artists:\", len(fma_df[\"artist\"][\"name\"].unique()))\n",
    "print(\"Number of albums:\", len(fma_df[\"album\"][\"title\"].unique()))\n",
    "\n",
    "# Let's print the number of unique genres\n",
    "print(\"Number of unique genres:\", len(fma_df[\"track\"][\"genre_top\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Again, we can use matplotlib to print some data distributions\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 20))\n",
    "\n",
    "# Plot the distribution of durations\n",
    "ax[0].hist(fma_df[\"track\"][\"duration\"], bins=100)\n",
    "ax[0].set_title(\"Duration distribution\")\n",
    "ax[0].set_xlabel(\"Duration (seconds)\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot the distribution of release data\n",
    "ax[1].hist(fma_df[\"album\"][\"date_released\"].dt.year, bins=100)\n",
    "ax[1].set_title(\"Release date distribution\")\n",
    "ax[1].set_xlabel(\"Release date (year)\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot only the distribution of genres\n",
    "\n",
    "# Remove unused categories first\n",
    "genre_series = fma_df[\"track\"][\"genre_top\"].cat.remove_unused_categories()\n",
    "\n",
    "ax[2].hist(genre_series.cat.codes, bins=len(genre_series.cat.categories))\n",
    "ax[2].set_title(\"Genre distribution\")\n",
    "ax[2].set_xlabel(\"Genre\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "ax[2].set_xticks(range(len(genre_series.cat.categories)), genre_series.cat.categories)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's listen to a song from the dataset\n",
    "audio_path = \"../datasets/fma_small/019/019073.mp3\"\n",
    "\n",
    "y, sr = librosa.load(audio_path, sr=44100)\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 2: RAVE\n",
    "\n",
    "![](./assets/rave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "# We'll start by downloading a model from the Hugging Face Hub\n",
    "# We'll use the `percussion.ts` model\n",
    "# from `lancelotblanchard/rave_percussion`\n",
    "\n",
    "model_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=\"shuoyang-zheng/jaspers-rave-models\",\n",
    "    filename=\"guitar_picking_dm_b2048_r44100_z8_causal.ts\",\n",
    "    cache_dir=\"../huggingface_hub_cache\",\n",
    "    force_download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We'll load our **compiled** model using torch.jit.load\n",
    "\n",
    "rave = torch.jit.load(model_path)\n",
    "print(rave) # This should print the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# We can use torchinfo to get a nicer looking summary of the model\n",
    "summary(rave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's load a sample audio file\n",
    "# nature beautiful bird calls by buzzatsea -- https://freesound.org/s/562864/\n",
    "# License: Creative Commons 0\n",
    "y, sr = librosa.load(\"assets/bird_calls.m4a\")\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We can use our RAVE model to encode the audio into a latent representation\n",
    "\n",
    "# First, we need to convert our audio to a PyTorch tensor and reshape it to the\n",
    "# required shape: (batch_size, n_channels, n_samples)\n",
    "audio = torch.from_numpy(y).float()\n",
    "audio = audio.reshape(1, 1, -1)\n",
    "\n",
    "# Now we can encode the audio using the model's `encode` method\n",
    "# Note: We need to use torch.no_grad() to avoid tracking gradients\n",
    "with torch.no_grad():\n",
    "    latent = rave.encode(audio)\n",
    "\n",
    "# We can print the shape of the latent representation\n",
    "# It should be of shape (batch_size, latent_dim, n_latent_codes)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just for fun, let's visualize the latent representation\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.imshow(latent[0].detach().numpy(), aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Latent Representation\")\n",
    "plt.xlabel(\"Latent Codes\")\n",
    "plt.ylabel(\"Latent Dimension\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now, let's decode the latent representation back to audio and listen to it\n",
    "# We can use the `decode` method to decode the latent representation\n",
    "# This should be again of shape (batch_size, n_channels, n_samples)\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_audio = rave.decode(latent)\n",
    "print(decoded_audio.shape) \n",
    "\n",
    "# Let's listen to the decoded audio\n",
    "display(Audio(decoded_audio[0].detach().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play around with the latent representation. One thing we can do is\n",
    "# to add some noise to the latent representation and see how it affects the\n",
    "# decoded audio. We'll use some Gaussian noise for this.\n",
    "\n",
    "noised_latent = latent + torch.randn_like(latent) * 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    noised_decoded_audio = rave.decode(noised_latent)\n",
    "    \n",
    "display(Audio(noised_decoded_audio[0].detach().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about decoding a completely random latent representation?\n",
    "# We'll generate a random tensor of the same shape as the latent representation\n",
    "\n",
    "random_latent = torch.randn_like(latent)\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_decoded_audio = rave.decode(random_latent)\n",
    "    \n",
    "display(Audio(random_decoded_audio[0].detach().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAVE Training\n",
    "\n",
    "Available at https://colab.research.google.com/drive/1aK8K186QegnWVMAhfnFRofk_Jf7BBUxl?usp=sharing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
