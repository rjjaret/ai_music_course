{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8: Diffusion Models for Music Generation\n",
    "\n",
    "Agenda:\n",
    "- Introduction to Diffusion Models\n",
    "- Conditioning & Classifier-Free Guidance\n",
    "- Hands On: Using Stable Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Diffusion Models\n",
    "\n",
    "Image example from _developer.nvidia.com_.\n",
    "\n",
    "![](./assets/diffusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Some concepts\n",
    "\n",
    "- $X_0$ is a random variable, distributed according to a distribution of interest\n",
    "that admits density $p_0$.\n",
    "\n",
    "- $X_t$ is a noisy version of $X_0$, where the noise is generated through a\n",
    "noise model:\n",
    "\n",
    "    $$\n",
    "    X_t = \\sqrt{1-\\sigma^2_t}X_0 + \\sigma_t Z, \\; \\; Z \\sim \\mathcal{N}(0, I_d)\n",
    "    $$,\n",
    "\n",
    "    where $\\sigma_0=0$, $\\sigma_t=\\sigma(t) \\in [0, 1]$ is an increasing\n",
    "    function of $t$ and $\\lim_{t \\to \\infty} \\sigma(t) = 1$.\n",
    "\n",
    "- We call $\\sigma(t)$ the _noise schedule_. It can be generated by a few\n",
    "different processes, e.g.:\n",
    "\n",
    "    $$\n",
    "    \\sigma(t) = \\sin \\Big( \\frac{t/T + s}{1 + s}\\frac{\\pi}{2} \\Big)\n",
    "    $$\n",
    "\n",
    "    <center><img src=\"assets/cosine_noise.png\" width=\"50%\"/></center>\n",
    "\n",
    "    In this case, we have a continuous time $t \\in \\mathbb{R}^+$. Some other\n",
    "    models might use a finite set of noise levels with $t \\in \\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corruption Process\n",
    " \n",
    "The _corruption process_ describes how the noise is added onto our data. We can\n",
    "use:\n",
    "\n",
    "1. **Variance Exploding:** $X_t = X_0 + \\sigma_t Z, \\; \\; \\sigma_t \\geq 0, \\lim_{t \\to \\infty} \\sigma_t = \\infty$\n",
    "\n",
    "2. **Variance Preserving:** $X_t = \\sqrt{1-\\sigma_t^2} X_0 + \\sigma_t Z, \\; \\; 0 \\leq \\sigma_t \\leq 1, \\lim_{t \\to \\infty} \\sigma_t = 1$\n",
    "\n",
    "3. **Flow Matching:** $X_t = (1 - \\sigma_t) X_0 + \\sigma_t Z, \\; \\; 0 \\leq \\sigma_t \\leq 1, \\lim_{t \\to \\infty} \\sigma_t = 1$\n",
    "\n",
    "The corruption process can also be described by a differential equation:\n",
    "\n",
    "$$\n",
    "dX_t = f(X_t, t)dt + g(t)dB\n",
    "$$,\n",
    "\n",
    "where $f(X_t,t)$ is the \"_drift_\", $g(t)$ is the _diffusion coefficient_, and\n",
    "$B_t$ is _Brownian noise_.\n",
    "\n",
    "**Important:** This process is _reversible_, and we have:\n",
    "\n",
    "$$\n",
    "d\\tilde{X}_t = \\big( -f(\\tilde{X}_t, T-t) + g^2(T-t) {\\color{orange} \\nabla \\log p_{T-t} (\\tilde{X}_t) } \\big) dt + g(T-t) d\\tilde{B}_t\n",
    "$$,\n",
    "\n",
    "where $X_t$ and $\\tilde{X}_t$ follow the same distribution, $B_t$ and\n",
    "$\\tilde{B}_t$ follow the same distribution, and ${\\color{orange} \\nabla \\log p_t(\\cdot)}$\n",
    "is the <span style=\"color: orange\">**score function**</span>.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Instead of approximating $p(X)$ directly like likelihood-based models, diffusion\n",
    "models try to approximate $\\nabla \\log p(X)$.\n",
    "\n",
    "</div>\n",
    "\n",
    "By moving in the direction of the score function, we can model a trajectory that\n",
    "guides to a sample of $p(X_0)$.\n",
    "\n",
    "![](./assets/sgld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The UNet Architecture\n",
    "\n",
    "From [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) and [Attention U-Net: Learning Where to Look for the Pancreas](https://arxiv.org/abs/1804.03999).\n",
    "\n",
    "![](./assets/unets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning & Classifier-Free Guidance\n",
    "\n",
    "From Bayes's Theorem, we have a conditional score function:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(X|Y) = \\nabla \\log p(Y|X) + \\nabla \\log p(X)\n",
    "$$\n",
    "\n",
    "This means that the **score function of our conditional model** is the sum of the _unconditional score function_, and a _conditioning term_. We usually scale our _conditioning term_ by a factor and get $\\gamma \\nabla \\log p(Y|X)$.\n",
    "\n",
    "We can either train a classifier to estimate $p(Y|X)$ (_Classifier Guidance_), or train our generative model to estimate both $p(X)$ and $p(X|Y)$ by using _conditioning dropout_, making our model capable of handling the conditioning signal when it is present. Using Bayes's Theorem, we further have that:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(Y|X) = \\nabla \\log p(X|Y) - \\nabla \\log p(X)\n",
    "$$\n",
    "\n",
    "Using our scaling factor, we have:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(X|Y) = (1-\\gamma) \\nabla \\log p(X) + \\gamma \\log p(X|Y)\n",
    "$$\n",
    "\n",
    "When $\\gamma = 0$, we have the unconditional model, and when $\\gamma = 1$, we have the conditional model. The interesting results of CFG happen when $\\gamma > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference-Time Optimization: DITTO\n",
    "\n",
    "From [DITTO: Diffusion Inference-Time T-Optimization for Music Generation](https://arxiv.org/abs/2401.12179).\n",
    "\n",
    "![](./assets/ditto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Hands On: Using Stable Audio\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**IMPORTANT:** Make sure you use the Python 3.10 venv for this Hands-On\n",
    "\n",
    "</div>\n",
    "\n",
    "### Using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Stability-AI/stable-audio-tools.git ../repositories/stable-audio-tools\n",
    "!pip install ../repositories/stable-audio-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import json\n",
    "import torch\n",
    "from stable_audio_tools.models.factory import create_model_from_config\n",
    "from stable_audio_tools.models.utils import load_ckpt_state_dict\n",
    "\n",
    "repo_id = \"stabilityai/stable-audio-open-1.0\"\n",
    "cache_dir = \"../huggingface_hub_cache\"\n",
    "\n",
    "model_config_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=\"model_config.json\",\n",
    "    cache_dir=cache_dir,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "model = create_model_from_config(model_config)\n",
    "\n",
    "model_ckpt_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=\"model.safetensors\",\n",
    "    cache_dir=cache_dir,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "model.load_state_dict(load_ckpt_state_dict(model_ckpt_path))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device).eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "sample_length = ...\n",
    "sample_size = ...\n",
    "conditioning = ...\n",
    "seed = ...\n",
    "\n",
    "generate_args = {\n",
    "    \"model\": ...,\n",
    "    \"conditioning\": ...,\n",
    "    \"negative_conditioning\": ...,\n",
    "    \"steps\": ...,\n",
    "    \"cfg_scale\": ...,\n",
    "    \"cfg_interval\": ...,\n",
    "    \"batch_size\": ...,\n",
    "    \"sample_size\": ...,\n",
    "    \"seed\": ...,\n",
    "    \"device\": ...,\n",
    "    \"sampler_type\": ...,\n",
    "    \"sigma_min\": ...,\n",
    "    \"sigma_max\": ...,\n",
    "    \"init_audio\": ...,\n",
    "    \"init_noise_level\": ...,\n",
    "    \"callback\": ...,\n",
    "    \"scale_phi\": ...,\n",
    "    \"rho\": ...\n",
    "}\n",
    "\n",
    "audio = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "y, sr = librosa.load(\"../session2_setup/assets/stargazing.wav\", sr=model.sample_rate)\n",
    "\n",
    "display(Audio(y, rate=sr))\n",
    "\n",
    "input_audio = torch.from_numpy(y).to(device)\n",
    "\n",
    "sample_length = ...\n",
    "sample_size = ...\n",
    "conditioning = ...\n",
    "seed = ...\n",
    "\n",
    "generate_args = {\n",
    "    \"model\": ...,\n",
    "    \"conditioning\": ...,\n",
    "    \"negative_conditioning\": ...,\n",
    "    \"steps\": ...,\n",
    "    \"cfg_scale\": ...,\n",
    "    \"cfg_interval\": ...,\n",
    "    \"batch_size\": ...,\n",
    "    \"sample_size\": ...,\n",
    "    \"seed\": ...,\n",
    "    \"device\": ...,\n",
    "    \"sampler_type\": ...,\n",
    "    \"sigma_min\": ...,\n",
    "    \"sigma_max\": ...,\n",
    "    \"init_audio\": ...,\n",
    "    \"init_noise_level\": ...,\n",
    "    \"callback\": ...,\n",
    "    \"scale_phi\": ...,\n",
    "    \"rho\": ...\n",
    "}\n",
    "\n",
    "cond_audio = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(cond_audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the downsampled latent size\n",
    "latent_size = ...\n",
    "\n",
    "# Set the seed for noise\n",
    "seed = ...\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Define the latent noise\n",
    "noise = ...\n",
    "print(noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some config\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the conditioner\n",
    "\n",
    "print(model.conditioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the conditioning tensors with the model's conditioner\n",
    "conditioning_tensors = ...\n",
    "\n",
    "print(conditioning_tensors[\"prompt\"])\n",
    "print(\"Shape of prompt embedding: \", conditioning_tensors[\"prompt\"][0].shape)\n",
    "print(\"Shape of seconds_start embedding: \", conditioning_tensors[\"seconds_start\"][0].shape)\n",
    "print(\"Shape of seconds_total embedding: \", conditioning_tensors[\"seconds_total\"][0].shape)\n",
    "# conditioning_tensors[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the cross attention conditioning inputs and the global conditioning\n",
    "# inputs\n",
    "conditioning_inputs = ...\n",
    "\n",
    "print(conditioning_inputs)\n",
    "print(\"Shape of the cross attention conditioning inputs: \", conditioning_inputs[\"cross_attn_cond\"].shape)\n",
    "print(\"Shape of the global conditioning inputs: \", conditioning_inputs[\"global_cond\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What features are used for the conditioning?\n",
    "print(\"Features used for cross attention:\", model.cross_attn_cond_ids)\n",
    "print(\"Features used for global conditioning:\", model.global_cond_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_audio_tools.inference.utils import prepare_audio\n",
    "\n",
    "io_channels = model.pretransform.io_channels\n",
    "\n",
    "init_audio = ...\n",
    "\n",
    "encoded_audio = model.pretransform.encode(init_audio)\n",
    "print(\"Encoded audio shape: \", encoded_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k_diffusion as K\n",
    "\n",
    "denoiser = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's use the noise schedule and plot the sigmas\n",
    "\n",
    "sigmas = ...\n",
    "\n",
    "# Scale the initial noise by the first sigma\n",
    "final_noise = ...\n",
    "\n",
    "# Add the encoded audio to the noise\n",
    "final_noise = ...\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_args = ...\n",
    "\n",
    "# Use dpmpp-3m-sde to sample the noise\n",
    "sampled = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sampled audio\n",
    "decoded_audio = ...\n",
    "\n",
    "display(Audio(decoded_audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play around with things!\n",
    "# First, let's look at different noise levels (0, 50, 100, 150, 200, 250)\n",
    "\n",
    "decoded_audio = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's experiment with a linear noise schedule\n",
    "linear_sigmas = ...\n",
    "\n",
    "sampled = ...\n",
    "decoded_audio = ...\n",
    "\n",
    "display(Audio(decoded_audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about starting with the input audio directly?\n",
    "\n",
    "sampled = ...\n",
    "decoded_audio = ...\n",
    "\n",
    "display(Audio(decoded_audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with less steps\n",
    "\n",
    "sigmas_20 = ...\n",
    "\n",
    "sampled = ...\n",
    "decoded_audio = ...\n",
    "\n",
    "display(Audio(decoded_audio.squeeze(0).cpu().numpy(), rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
