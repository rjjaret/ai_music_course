{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Autoregressive Music Generation (Part 2)\n",
    "\n",
    "Agenda\n",
    "- Understanding MusicGen\n",
    "- Hands On: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MusicGen\n",
    "\n",
    "From the paper [Simple and Controllable Music Generation (Copet et al., 2023)](https://arxiv.org/abs/2306.05284).\n",
    "\n",
    "![](./assets/musicgen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On: Using MusicGen to generate audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache\")\n",
    "\n",
    "from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = MusicgenMelodyProcessor.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can first generate unconditional music\n",
    "unconditional_inputs = processor.get_unconditional_inputs(num_samples=1).to(model.device)\n",
    "unconditional_audio_values =  model.generate(**unconditional_inputs, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's listen to our audio\n",
    "\n",
    "display(Audio(unconditional_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate a piece of music conditionally, with a given text prompt\n",
    "\n",
    "text_conditioned_inputs = processor(\n",
    "    text=['90s grunge track with gritty guitar strumming'],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# text_conditioned_audio_values = model.generate(**text_conditioned_inputs, guidance_scale=3)\n",
    "\n",
    "\n",
    "display(Audio(text_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# And we can also generate with a melody condition, passed as an audio array\n",
    "\n",
    "y, sr = librosa.load(\"assets/bolero_ravel.mp3\", sr=model.config.sampling_rate)\n",
    "\n",
    "display(Audio(y, rate=sr))\n",
    "\n",
    "melody_conditioned_inputs = processor(\n",
    "    audio=y,\n",
    "    sampling_rate=model.config.sampling_rate,\n",
    "    text=['bluegrass americana guitar, fiddle, bass, percussion'],\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "melody_conditioned_audio_values = model.generate(**melody_conditioned_inputs, guidance_scale=3, max_new_tokens=512)\n",
    "display(Audio(melody_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))\n",
    "\n",
    "melody_conditioned_audio_values = model.generate(**melody_conditioned_inputs, guidance_scale=3, max_new_tokens=512)\n",
    "display(Audio(melody_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))\n",
    "\n",
    "melody_conditioned_audio_values = model.generate(**melody_conditioned_inputs, guidance_scale=3, max_new_tokens=512)\n",
    "display(Audio(melody_conditioned_audio_values.squeeze(0, 1).cpu(), rate=model.config.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this model actually generates music\n",
    "\n",
    "# ############### #\n",
    "# 0. CONDITIONING # \n",
    "# ############### #\n",
    "\n",
    "text_prompt = processor.tokenizer(\"1970s rock/reggae fusion like the band The Police\", return_tensors=\"pt\")\n",
    "inputs_tensor = text_prompt[\"input_ids\"].to(model.device)\n",
    "attention_mask = text_prompt[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(inputs_tensor)\n",
    "print(attention_mask)\n",
    "\n",
    "# Then, we get our melody conditioning (a chroma spectrogram)\n",
    "melody_prompt = processor.feature_extractor(y, sampling_rate=model.config.sampling_rate, return_tensors=\"pt\")\n",
    "input_features = melody_prompt[\"input_features\"].to(model.device)\n",
    "\n",
    "print(melody_prompt[\"input_features\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ################# #\n",
    "# 1. PREPARE CONFIG #\n",
    "# ################# #\n",
    "\n",
    "generation_config = copy.deepcopy(model.generation_config)\n",
    "model._prepare_special_tokens(generation_config, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# #################### #\n",
    "# 2. TEXT CONDITIONING #\n",
    "# #################### #\n",
    "\n",
    "encoder = model.get_text_encoder()\n",
    "\n",
    "# get last hidden state to pass to decoder for cross attention\n",
    "with torch.no_grad():\n",
    "    encoder_hidden_states = encoder(\n",
    "        input_ids=inputs_tensor,\n",
    "        attention_mask = attention_mask,\n",
    "        output_attentions = generation_config.output_attentions,\n",
    "        output_hidden_states=generation_config.output_hidden_states\n",
    "    ).last_hidden_state\n",
    "\n",
    "# project encoder_hidden_states\n",
    "print(encoder_hidden_states.shape)\n",
    "encoder_hidden_states = model.enc_to_dec_proj(encoder_hidden_states)\n",
    "print(encoder_hidden_states.shape)\n",
    "\n",
    "# for classifier free guidance we need to add a 'null' input to our encoder hidden states - includes version without condition, so need null inputs added\n",
    "encodeer_hidden_states = torch.concatenate([encoder_hidden_states, torch.zeros_like(encoder_hidden_states)])\n",
    "encoder_attention_mask = torch.concatenate(\n",
    "    [attention_mask, torch.zeros_like(attention_mask)]\n",
    "\n",
    ")\n",
    "encoder_hidden_states = encoder_hidden_states * encoder_attention_mask[..., None]\n",
    "\n",
    "print(encoder_hidden_states.shape) #now has two batches, one for with and without the conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################### #\n",
    "# 3. AUDIO CONDITIONING #\n",
    "# ##################### #\n",
    "\n",
    "print(input_features.shape)\n",
    "\n",
    "null_audio_hidden_states = torch.zeros_like(input_features)\n",
    "null_audio_hidden_states[:,:,0]=1\n",
    "#\n",
    "# for classifier free guidance we need to add a 'null' input to our audio hidden states\n",
    "audio_hidden_states = torch.concatenate([input_features, null_audio_hidden_states], dim=0)\n",
    "\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "# project audio_hidden_states ->\n",
    "# (batch_size, seq_len, num_chroma) -> (batch_size, seq_len, hidden_size)\n",
    "audio_hidden_states = torch.concatenate([input_features, null_audio_hidden_states], dim=0)\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "audio_hidden_states = model.audio_enc_to_dec_proj(audio_hidden_states)\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "# pad or truncate to config.chroma_length\n",
    "n_repeat = int(math.ceil(model.config.chroma_length / audio_hidden_states.shape[1]))\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "audio_hidden_states = audio_hidden_states = audio_hidden_states.repeat(1, n_repeat,1)\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "audio_hidden_states = audio_hidden_states[:, :model.config.chroma_length]\n",
    "print(audio_hidden_states.shape)\n",
    "\n",
    "encoder_hidden_states = torch.cat([audio_hidden_states, encoder_hidden_states], dim=1)\n",
    "print(encoder_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################### #\n",
    "# 4. PREPARE AUTO-REGRESSIVE GENERATION #\n",
    "# ##################################### #\n",
    "\n",
    "input_ids = torch.tensor([\n",
    "    [2048],\n",
    "    [2048],\n",
    "    [2048],\n",
    "    [2048]\n",
    "], device=model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### #\n",
    "# 5. BUILD DELAY PATTERN #\n",
    "# ###################### #\n",
    "\n",
    "max_length = 513\n",
    "\n",
    "input_ids, decoder_delay_pattern_mask = model.decoder.build_delay_pattern_mask(\n",
    "    input_ids,\n",
    "    pad_token_id=generation_config._decoder_start_token_tensor,\n",
    "    max_length=max_length,\n",
    ")\n",
    "print(decoder_delay_pattern_mask.shape)\n",
    "print(decoder_delay_pattern_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClassifierFreeGuidanceLogitsProcessor, LogitsProcessorList, TopKLogitsWarper\n",
    "\n",
    "# ########################### #\n",
    "# 6. PREPARE LOGITS PROCESSOR #\n",
    "# ########################### #\n",
    "\n",
    "guidance_scale = 3\n",
    "\n",
    "logits_processor = LogitsProcessorList()\n",
    "logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(guidance_scale))\n",
    "logits_processor.append(TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# ############################ #\n",
    "# 7. PREPARE STOPPING CRITERIA #\n",
    "# ############################ #\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList()\n",
    "stopping_criteria.append(MaxLengthCriteria(max_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### #\n",
    "# 8. RUN SAMPLING LOOP #\n",
    "# #################### #\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model._sample(input_ids, logits_processor = logits_processor, stopping_criteria=stopping_criteria,\n",
    "                            generation_config=generation_config, use_cache=True, guidance_scale=guidance_scale,\n",
    "                            input_features=input_features, encoder_hiden_states=encoder_hidden_states,\n",
    "                            decoder_delay_pattern_mask=decoder_delay_pattern_mask,\n",
    "                            synced_gpus=None, streamer=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################ #\n",
    "# 9. DECODE OUTPUT #\n",
    "# ################ #\n",
    "\n",
    "# apply the pattern mask to the final ids\n",
    "output_ids = model.decoder.apply_delay_pattern_mask(outputs, decoder_delay_pattern_mask)\n",
    "\n",
    "# revert the pattern delay mask by filtering the pad token id\n",
    "output_ids = output_ids[output_ids!=generation_config._pad_token_tensor].reshape(1, model.decoder.num_codebooks, -1)\n",
    "\n",
    "# append the frame dimension back to the audio codes\n",
    "output_ids = output_ids[None, ...]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_values = model.audio_encoder.decode(output_ids, audio_scales=[None],).audio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we get a similar output?\n",
    "\n",
    "display(Audio(output_values.cpu().squeeze(0, 1), rate=model.config.sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
