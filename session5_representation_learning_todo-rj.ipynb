{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: Representation Learning for Music\n",
    "\n",
    "**Agenda:**\n",
    "\n",
    "- Comparison of Music representations\n",
    "- Case Study: Encodec\n",
    "- Hands On 1: Using Encodec\n",
    "- Hands On 2: Encoding MIDI\n",
    "- Hands On 3: Using HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of some representations\n",
    "\n",
    "From [Comparing Representations for Audio Synthesis Using Generative Adversarial Networks (Nistal, Lattner, Richard, 2020)](https://arxiv.org/abs/2006.09266).\n",
    "\n",
    "![](./assets/comparison_representations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Encodec\n",
    "\n",
    "Encodec is a *neural audio compression framework* developed by Meta, that\n",
    "enables the efficient compression of high-fidelity audio into a compact discrete\n",
    "representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/encodec_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Residual Vector Quantization (RVQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/rvq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of this method is that it takes $N_q$ steps (with many lookups!) to\n",
    "quantize one single vector. So this becomes very costly to quantize an entire \n",
    "tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encodec speedup: RVQ through Transformers\n",
    "\n",
    "![](./assets/rvq_transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 1: Using Encodec to encode and decode audio waveforms\n",
    "\n",
    "We can directly use the `EncodecModel` provided by the `transformers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"../huggingface_hub_cache/\")\n",
    "\n",
    "from transformers import EncodecModel, EncodecFeatureExtractor\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = EncodecFeatureExtractor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Again, let's get some info using torchinfo\n",
    "summary(model, input_data=[torch.randn(1, 1, 320)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we get this downsampling factor value? We can just multiply all of the\n",
    "# strides!\n",
    "downsampling_factor = 1\n",
    "\n",
    "for layer in model.encoder.layers:\n",
    "    if hasattr(layer, \"stride\"):\n",
    "        downsampling_factor *= layer.stride.item()\n",
    "\n",
    "print(f\"Downsampling factor: {downsampling_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Let's load the audio file\n",
    "y, sr = librosa.load(\"../session2_setup/assets/stargazing.wav\", sr=processor.sampling_rate)\n",
    "\n",
    "# Let's first go through the processor\n",
    "inputs=processor(raw_audio=y, sampling_rate=sr,return_tensors=\"pt\")\n",
    "\n",
    "# Let's select the lowest bandwidth\n",
    "bandwidth = model.config.target_bandwidths[0]\n",
    "print(f\"Target bandwidth: {bandwidth} kbps\")\n",
    "\n",
    "# We can now pass the inputs to the model to get the encoder outputs\n",
    "encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], bandwidth)\n",
    "\n",
    "# Let's get the shape of the encoder outputs\n",
    "print(encoder_outputs.audio_codes.shape)\n",
    "\n",
    "# Let's calculate the compression ratio\n",
    "compression_ratio = y.shape[0] / encoder_outputs.audio_codes.shape[-1]\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's decode those codes back to audio\n",
    "audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0].squeeze(0, 1)\n",
    "\n",
    "display(Audio(audio_values.detach().numpy(), rate=processor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it one more time with a higher bandwidth!\n",
    "bandwidth_high = model.config.target_bandwidths[-1]\n",
    "print(f\"Target bandwidth: {bandwidth_high} kbps\")\n",
    "\n",
    "encoder_outputs_high = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], bandwidth_high)\n",
    "print(f\"Encoder outputs shape: {encoder_outputs_high.audio_codes.shape}\")\n",
    "\n",
    "# Let's decode those codes back to audio\n",
    "audio_values_high = model.decode(encoder_outputs_high.audio_codes, encoder_outputs_high.audio_scales, inputs[\"padding_mask\"])[0].squeeze(0,1)\n",
    "\n",
    "display(Audio(audio_values_high.detach().numpy(), rate=processor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print some of the codes\n",
    "print(encoder_outputs_high.audio_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's dive a bit through the encoder (before the quantization)\n",
    "pre_quantized = model.encoder(inputs[\"input_values\"]*inputs[\"padding_mask\"].unsqueeze(1))\n",
    "\n",
    "print(f\"Pre-quantized shape: {pre_quantized.shape}\")\n",
    "print(f\"Hidden size of the encoder: {model.config.hidden_size}\")\n",
    "\n",
    "print(pre_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the outputs of the first codebook and print them\n",
    "cb1 = model.quantizer.layers[0].codebook.quantize(pre_quantized.permute(0,2,1).reshape(-1, model.config.hidden_size))\n",
    "print(cb1)\n",
    "# print(\"Orig: \", pre_quantized.permute(0,2,1).Shape)\n",
    "# print(\"Orig: \", pre_quantized.permute(0,2,1).Shape)\n",
    "# print(\"Reshape: \", pre_quantized.permute(0,2,1).reshape(-1, model.config.hidden_size).Shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How far away are we? Let's decode the first code and compare to our first vector\n",
    "x_hat = torch.nn.functional.embedding(cb1[0], model.quantizer.layers[0].codebook.embed)\n",
    "print(x_hat.shape)\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "dist = torch.norm(pre_quantized[0, :, 0] - x_hat)\n",
    "print(f\"Euclidean distance: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it the best we can do? Let's verify whether `quantize` gave us the best code\n",
    "distances = []\n",
    "for i in range(model.config.codebook_size):\n",
    "    x_hat = torch.nn.functional.embedding(torch.tensor(i), model.quantizer.layers[0].codebook.embed)\n",
    "    dist = torch.norm(pre_quantized[0,:,0] - x_hat)\n",
    "    distances.append(dist.item())\n",
    "\n",
    "argmin = torch.argmin(torch.tensor(distances))\n",
    "\n",
    "print(f\"Minimum distance is {distances[argmin]:.4f} for code {argmin.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the outputs of the SECOND codebook, and print them.\n",
    "#\n",
    "residual = pre_quantized - (torch.nn.functional.embedding(cb1.unsqueeze(0), model.quantizer.layers[0].codebook.embed).permute(0,2,1))\n",
    "\n",
    "cb2 = model.quantizer.layers[1].codebook.quantize(residual.permute(0,2,1).reshape(-1,model.config.hidden_size))\n",
    "print(cb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check whether we improved the distance\n",
    "x_hat2 = torch.nn.functional.embedding(cb2[0], model.quantizer.layers[1].codebook.embed)\n",
    "print(x_hat2.shape)\n",
    "\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "dist = torch.norm(pre_quantized[0,:,0] - (x_hat + x_hat2))\n",
    "print(f\"Euclidean distance: {dist:.4f}\") # hope that this is smaller than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the final distance\n",
    "n_codebooks = model.quantizer.get_num_quantizers_for_bandwidth(bandwidth_high)\n",
    "\n",
    "all_xhats = torch.zeros(n_codebooks, model.config.hidden_size)\n",
    "\n",
    "for i in range(n_codebooks):\n",
    "    xhat = torch.nn.functional.embedding(encoder_outputs_high.audio_codes[0,0,i,0], model.quantizer.layers[i].codebook.embed)\n",
    "    all_xhats[i]=xhat\n",
    "\n",
    "# Let's calculate the Euclidean distance between the two vectors\n",
    "final_dist = torch.norm(pre_quantized [0,:,0] - torch.sum(all_xhats, dim=0))\n",
    "# print(final_dist)\n",
    "print(f\"Euclidean distance: {final_dist:.4f}\") # hope that this is smaller than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's plot how distance decreases with higher bandwidth / more codebooks\n",
    "distances = []\n",
    "\n",
    "for i in range(n_codebooks):\n",
    "    distances.append(torch.norm(pre_quantized[0,:,0] - torch.sum(all_xhats[:i+1], dim=0)).item())\n",
    "\n",
    "target_codebooks = [model.quantizer.get_num_quantizers_for_bandwidth(n) for n in model.config.target_bandwidths]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, n_codebooks + 1), distances)\n",
    "plt.vlines(target_codebooks, ymin=0, ymax=max(distances), color=\"grey\", linestyles=\"dashed\")\n",
    "plt.title(\"Euclydian Distances Based on Number of Codebooks\")\n",
    "plt.xlabel(\"Number of Codebooks\")\n",
    "plt.ylabel(\"Euclydian Distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 2: Encoding MIDI using the Anticipatory Music Transformer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "from collections import defaultdict\n",
    "\n",
    "# This is our time resolution (how many ticks per second)\n",
    "TIME_RESOLUTION = 100\n",
    "\n",
    "# Let's first load the MIDI file\n",
    "midi = mido.MidiFile(\"../session2_setup/assets/symphony40.mid\")\n",
    "\n",
    "def midi_to_tokens(midi):\n",
    "    # We will keep track of time, tokens, note index, open note ons, and instruments\n",
    "    time = 0\n",
    "    tokens = []\n",
    "    note_idx = 0\n",
    "    open_notes = defaultdict(list)\n",
    "    instruments = defaultdict(list)\n",
    "\n",
    "    # We iterate through the MIDI file\n",
    "    for message in midi:\n",
    "        # We add the message time to our global clock\n",
    "        time += message.time\n",
    "\n",
    "        # If we have a program change, add a new instrument\n",
    "        if message.type == \"program_change\":\n",
    "            instruments[message.channel] = message.program\n",
    "        \n",
    "        elif message.type == \"note_on\" and message.velocity > 0:\n",
    "            time_in_ticks = round(TIME_RESOLUTION*time)\n",
    "\n",
    "            # Add (time, duration, note, instrument, velocity) to tokens\n",
    "            tokens.append(time_in_ticks)\n",
    "            tokens.append(-1) # We do not have the duration yet \n",
    "            tokens.append(message.note)\n",
    "            tokens.append(instruments[message.channel])\n",
    "            tokens.append(message.velocity)\n",
    "\n",
    "            # Keep track of open Note On\n",
    "            open_notes[(instruments[message.channel], message.note, message.channel)].append((note_idx, time))\n",
    "            note_idx += 1\n",
    "\n",
    "        elif message.type == \"note_off\":\n",
    "            try:\n",
    "                open_idx, onset_time = open_notes[(instruments[message.channel], message.note, message.channel)].pop(0)\n",
    "            except IndexError:\n",
    "                print(\"WARNING: Note off before note on!\")\n",
    "            else:\n",
    "                duration_in_ticks = round(TIME_RESOLUTION*(time - onset_time))\n",
    "                tokens[5*open_idx + 1] = duration_in_ticks\n",
    "\n",
    "    # At the end of the conversion, check how many notes are still open\n",
    "    unclosed_count = 0\n",
    "    for _, v in open_notes.items():\n",
    "        unclosed_count += len(v)\n",
    "\n",
    "    if unclosed_count > 0:\n",
    "        print(f\"WARNING: {unclosed_count} unclosed notes\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# We can call our function\n",
    "tokens = midi_to_tokens(midi)\n",
    "print(f\"Length of sequence: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, bpm=120):\n",
    "    midi = mido.MidiFile()\n",
    "    midi.ticks_per_beat = (60 * TIME_RESOLUTION) // bpm\n",
    "\n",
    "    # We will create a dictionary that maps (time_in_ticks, event_type) to\n",
    "    # (note, instrument, velocity) with event_type=0 for Note On and\n",
    "    # event_type=1 for Note Off\n",
    "    time_idx = defaultdict(list)\n",
    "    for (time_in_ticks, duration_in_ticks, note, instrument, velocity) in zip(tokens[0::5], tokens[1::5], tokens[2::5], tokens[3::5], tokens[4::5]):\n",
    "        time_idx[(time_in_ticks, 0)].append((note, instrument, velocity))\n",
    "        time_idx[(time_in_ticks+duration_in_ticks, 1)].append((note, instrument, 0))\n",
    "\n",
    "    # track_idx maps instruments to (track, previous_time, idx)\n",
    "    track_idx = {}\n",
    "    num_tracks = 0\n",
    "\n",
    "    # Double loop to go through all events\n",
    "    for time_in_ticks, event_type in sorted(time_idx.keys()):\n",
    "        for (note, instrument, velocity) in time_idx[(time_in_ticks, event_type)]:\n",
    "            # If Note On, add Note On to the track\n",
    "            if event_type == 0:\n",
    "                try:\n",
    "                    track, previous_time, idx = track_idx[instrument]\n",
    "                except KeyError:\n",
    "                    # If it doesn't exist, add it!\n",
    "                    track = mido.MidiTrack()\n",
    "                    previous_time = 0\n",
    "                    idx = num_tracks\n",
    "\n",
    "                    # Add the track to our MIDI file\n",
    "                    midi.tracks.append(track)\n",
    "\n",
    "                    # Create a program_change event and add it to the track\n",
    "                    message = mido.Message(\"program_change\", channel=idx, program=instrument)\n",
    "                    track.append(message)\n",
    "                    num_tracks += 1\n",
    "                finally:\n",
    "                    track.append(mido.Message(\"note_on\", note=note, channel=idx, velocity=velocity, time=time_in_ticks-previous_time))\n",
    "                    track_idx[instrument] = (track, time_in_ticks, idx)\n",
    "            \n",
    "            # If Note Off, add Note Off to the track\n",
    "            elif event_type == 1:\n",
    "                try:\n",
    "                    track, previous_time, idx = track_idx[instrument]\n",
    "                except KeyError:\n",
    "                    # If it doesn't exist, there is a problem\n",
    "                    print(f\"WARNING: Note Off for note {note} and instrument {instrument} before Note On\")\n",
    "                else:\n",
    "                    track.append(mido.Message(\"note_off\", note=note, channel=idx, time=time_in_ticks-previous_time))\n",
    "                    track_idx[instrument] = (track, time_in_ticks, idx)\n",
    "            \n",
    "    return midi\n",
    "\n",
    "# We can call our function\n",
    "reconstructed_midi = tokens_to_midi(tokens)\n",
    "reconstructed_midi.save(\"assets/reconstructed_midi.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi2audio\n",
    "\n",
    "# We can listen to our reconstructed MIDI\n",
    "midi2audio_obj = midi2audio.FluidSynth(\"../session2_setup/assets/soundfont.sf2\")\n",
    "midi2audio_obj.midi_to_audio(\"assets/reconstructed_midi.mid\", \"assets/reconstructed_midi.wav\")\n",
    "\n",
    "y, sr = librosa.load(\"assets/reconstructed_midi.wav\")\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On 3: Using HiFi-GAN to synthesize speech spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HiFi-GAN repository\n",
    "!git clone https://github.com/jik876/hifi-gan.git ../repositories/hifi-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "# Download pretrained model from `lancelotblanchard/hifi_gan_vctk_v3`\n",
    "\n",
    "model_path = huggingface_hub.snapshot_download(\n",
    "    repo_id=\"lancelotblanchard/hifi_gan_vctk_v3\",\n",
    "    cache_dir=\"../huggingface_hub_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the path to the repository to the system path\n",
    "import sys\n",
    "sys.path.append(\"../repositories/hifi-gan\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from env import AttrDict\n",
    "\n",
    "config_file = os.path.join(model_path, \"config.json\")\n",
    "with open(config_file) as f:\n",
    "    h = AttrDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the config\n",
    "print(json.dumps(h, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator\n",
    "import torch\n",
    "\n",
    "generator = Generator(h)\n",
    "state_dict_g = torch.load(os.path.join(model_path, \"generator_v3\"), map_location=\"cpu\")[\"generator\"]\n",
    "generator.load_state_dict(state_dict_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Let's grab an audio file\n",
    "y, sr = librosa.load(\"../session2_setup/assets/stargazing.wav\")\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.from_numpy(y).reshape(1, -1)\n",
    "\n",
    "n_fft = h[\"n_fft\"]\n",
    "num_mels = h[\"num_mels\"]\n",
    "fmin = h[\"fmin\"]\n",
    "fmax = h[\"fmax\"]\n",
    "win_size = h[\"win_size\"]\n",
    "hop_size = h[\"hop_size\"]\n",
    "\n",
    "# we use librosa's mel() function to create the mel filterbank\n",
    "mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "mel = torch.from_numpy(mel).float()\n",
    "\n",
    "# We create a Hann window of size win_size.\n",
    "# The Hann window is used to smooth the signal before applying the STFT.\n",
    "hann_window = torch.hann_window(win_size)\n",
    "\n",
    "# We pad the audio signal to make sure that the length is a multiple of hop_size\n",
    "audio = torch.nn.functional.pad(audio, (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)), mode=\"reflect\")\n",
    "audio = audio.squeeze(1)\n",
    "\n",
    "# We compute the STFT of the audio signal\n",
    "spec = torch.view_as_real(torch.stft(audio, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window,\n",
    "                  center=False, pad_mode=\"reflect\", normalized=False, onesided=True, return_complex=True))\n",
    "\n",
    "# We compute the magnitude of the STFT\n",
    "spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n",
    "\n",
    "# Finally, we compute the mel spectrogram by applying the mel filterbank to the\n",
    "# magnitude of the STFT.\n",
    "spec = torch.matmul(mel, spec)\n",
    "\n",
    "# We normalize the mel spectrogram to the range [0, 1]\n",
    "spec = torch.log(torch.clamp(spec, min=1e-5) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reset the matplotlib backend to use the inline backend\n",
    "import matplotlib\n",
    "from importlib import reload\n",
    "matplotlib = reload(matplotlib)\n",
    "matplotlib.use(\"inline\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Let's plot the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spec[0].detach().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Mel spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_g_hat = generator(spec)\n",
    "\n",
    "audio2 = y_g_hat.squeeze()\n",
    "\n",
    "display(Audio(audio2.detach().numpy(), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
