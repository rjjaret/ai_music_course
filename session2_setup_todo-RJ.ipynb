{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Setting up your environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "1. This GitHub repository (`lancelotblanchard/ai_music_course`)\n",
    "2. A Python Version Management system (e.g., pyenv or conda)\n",
    "3. Python 3.12 with a virtual environment `.venv`\n",
    "4. All of the dependencies in `requirements.txt`\n",
    "5. Some external libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cloning the GitHub repository\n",
    "\n",
    "- Install `git` (version control system)\n",
    "\n",
    "- Clone https://github.com/lancelotblanchard/ai_music_course.git\n",
    "\n",
    "- Your file structure should look like this:\n",
    "\n",
    "<img src=\"./assets/structure.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing PyEnv\n",
    "\n",
    "- Following the instructions at https://github.com/pyenv/pyenv (e.g., through `curl -fsSL https://pyenv.run | bash`)\n",
    "\n",
    "- Install Python 3.12 through `pyenv install 3.12`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Python environment\n",
    "\n",
    "- Create a virtual environment (`python -m venv .venv`)\n",
    "- Activate it (`. .venv/bin/activate`)\n",
    "- Install the requirements (`pip install -r requirements.txt`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Installing the required programs\n",
    "\n",
    "- Install FluidSynth (https://github.com/FluidSynth/fluidsynth/wiki/Download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On 1: Manipulating Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading, visualizing, playing audio\n",
    "\n",
    "The audio file we use (Phish Funk - Stargazing) is distributed by Free Music\n",
    "Archive under a CC BY-NC license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# We can load an audio file using librosa and print its sample rate and the \n",
    "# shape of the audio data\n",
    "\n",
    "y, sr = librosa.load(\"assets/stargazing.wav\")\n",
    "print(\"Loaded audio with sample rate:\", sr)\n",
    "print(\"Audio shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate the duration by calculating (number of samples / sample rate)\n",
    "total_samples = y.shape[0]\n",
    "duration = total_samples//sr\n",
    "print(\"Audio duration (seconds):\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# We can visualize the audio waveform using matplotlib\n",
    "# The x-axis will be the sample number and the y-axis will be the amplitude\n",
    "time=numpy.arange(0, duration, 1/sr)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(time, y)\n",
    "plt.title('Wave Amplitude')\n",
    "plt.xlabel='Seconds'\n",
    "plt.ylabel=\"Amplitude\"\n",
    "plt.xlim= (0, duration)\n",
    "plt.ylim = (-1,1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# If we want to visualize the audio waveform in a more meaningful way, we can\n",
    "# convert the sample number to time in seconds.\n",
    "\n",
    "# The time axis can be created by creating an array from 0 to the duration\n",
    "# with a step size of 1/sr (= the time between samples)\n",
    "time = numpy.arange(0, duration, 1/sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(y, rate=sr))\n",
    "\n",
    "# How does our audio sound? We can use IPython's Audio class to create an Audio\n",
    "# widget that can play the audio in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting audio features\n",
    "\n",
    "`librosa` offers us a toolkit to extract audio features from the audio data.\n",
    "Most features are calculated over *sliding windows* on our audio data. These\n",
    "windows are parameterized with a *frame length* and a *hop length*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "\n",
    "# With a given hop length, we can calculate the number of frames in the audio data\n",
    "n_frames = 1 + total_samples/hop_length\n",
    "print(\"Number of frames:\", n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Root Mean Square\n",
    "\n",
    "There are many features we can extract from audio data. To get a sense of how\n",
    "the volume of the audio changes over time, we can calculate the Root Mean\n",
    "Square (RMS) energy of the audio signal. RMS is a measure of the average power\n",
    "of the signal, and it can be used to estimate the perceived loudness of the\n",
    "audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)\n",
    "print(\"RMS shape:\", rms.shape) # we should get the same number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our RMS signal using matplotlib. We can use the same time axis\n",
    "# we created before, but we need to make sure to adjust the time axis to match\n",
    "# the number of frames. The time axis for the RMS signal can be created by \n",
    "# creating an array from 0 to the duration with a step size of hop_length/sr\n",
    "# (= the time between frames). We'll display both the RMS signal and the original\n",
    "# audio waveform in the same diagram.\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Original audio\n",
    "p=ax[0]\n",
    "p.plot(time, y)\n",
    "p.set_title(\"Audio Wave\")\n",
    "p.set_xlabel(\"Seconds\")\n",
    "p.set_ylabel(\"Amplitude\")\n",
    "p.set_xlim(0, duration)\n",
    "p.set_ylim(-1, 1)\n",
    "p.grid()\n",
    "# p.show()\n",
    "\n",
    "p=ax[1]\n",
    "rms_time=numpy.arange(0, duration, hop_length/sr)\n",
    "\n",
    "p.plot(rms_time, rms[0])\n",
    "p.set_title(\"RMS\")\n",
    "p.set_xlabel(\"Seconds\")\n",
    "p.set_ylabel(\"Amplitude\")\n",
    "p.set_xlim(0, duration)\n",
    "p.set_ylim(0, 1)\n",
    "p.grid()\n",
    "# p.show()\n",
    "# RMS signal\n",
    "...\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the beats concide with high RMS values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Zero-crossing Rate\n",
    "\n",
    "Another feature we can extract is the Zero Crossing Rate (ZCR). The ZCR is the\n",
    "rate at which the signal changes from positive to negative or vice versa. It\n",
    "represents the frequency of the signal and is generally used to detect\n",
    "percussions, vocal activity, and other transient sounds. The ZCR is calculated\n",
    "by counting the number of times the signal crosses zero in a given frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr = librosa.feature.zero_crossing_rate(y=y, frame_length=frame_length, hop_length=hop_length)\n",
    "print(\"ZCR shape:\", zcr.shape) # we should get the same number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same method as before to visualize the ZCR signal.\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Original audio\n",
    "p=ax[0]\n",
    "p.plot(time, y)\n",
    "p.set_title(\"Audio Wave\")\n",
    "p.set_xlabel(\"Seconds\")\n",
    "p.set_ylabel(\"Amplitude\")\n",
    "p.set_xlim(0, duration)\n",
    "p.set_ylim(-1, 1)\n",
    "p.grid()\n",
    "\n",
    "# ZCR signal\n",
    "p=ax[1]\n",
    "p.plot(rms_time, zcr[0])\n",
    "p.set_title(\"Zero Crossing Points\")\n",
    "p.set_xlabel(\"Seconds\")\n",
    "p.set_ylabel(\"Amplitude\")\n",
    "p.set_xlim(0, duration)\n",
    "p.set_ylim(-1, 1)\n",
    "p.grid()\n",
    "\n",
    "...\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Spectrogram via Short-Time Fourier Transform\n",
    "\n",
    "A core concept in audio processing is the Short-Time Fourier Transform (STFT).\n",
    "The STFT is a way to represent a signal in the time-frequency domain (see\n",
    "picture). It allows us to separate a signal into all of its frequency\n",
    "components and see how they change over time. The STFT is calculated by\n",
    "taking a window of the signal, applying a Fourier Transform to it, and\n",
    "sliding the window over the signal. The result is a 2D array where the\n",
    "rows represent the frequency bins and the columns represent the time frames.\n",
    "The STFT is a complex-valued array (where the complex component represents the\n",
    "phase of the signal), so we can use the magnitude of the STFT to only keep\n",
    "the amplitude information.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTYCtcx_7otHVu-uToI9dA.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048 # number of samples in the FFT window\n",
    "\n",
    "# We can calculate the STFT using librosa's stft function. The STFT is a\n",
    "# complex-valued matrix of shape (n_fft/2+1, n_frames) that contains the\n",
    "# complex-valued STFT coefficients for each frame.\n",
    "stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "print(\"STFT shape:\", stft.shape) # we should get (n_fft/2+1, n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the STFT using matplotlib. We will use the imshow function to\n",
    "# create a heatmap of the STFT coefficients. The x-axis will be the frame number\n",
    "# and the y-axis will be the frequency bin number. The color of each pixel will\n",
    "# represent the magnitude of the STFT coefficient at that frequency and time.\n",
    "\n",
    "# We will use librosa's amplitude_to_db function to convert the STFT\n",
    "# coefficients to decibels. The decibel scale is a logarithmic scale that is\n",
    "# closely related to the human perception of sound.\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(librosa.amplitude_to_db(numpy.abs(stft),ref=numpy.max), aspect='auto', origin='lower', extent=[0,duration, 0, sr/2])\n",
    "plt.title('FFT')\n",
    "plt.xlabel='Frequency'\n",
    "plt.ylabel='Time'\n",
    "plt.xlim=(0, duration)\n",
    "plt.ylim=(0, sr/2)\n",
    "plt.colorbar(format='%+2.0f db')\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "\n",
    "Another feature we can extract from audio data is the Mel-frequency cepstral\n",
    "coefficients (MFCCs). MFCCs are a representation of the short-term power\n",
    "spectrum of sound, and they are commonly used in speech and audio processing\n",
    "tasks. They are based on the human ear's perception of sound, and they can\n",
    "be used to represent the timbre of an audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 13 # number of MFCCs to compute\n",
    "\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "print(\"MFCC shape:\", mfcc.shape) # we should get the same number of frames, with n_mfcc rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs are usually visualized with heatmaps, where the x-axis is the frame\n",
    "# number and the y-axis is the MFCC number. The color intensity represents the\n",
    "# value of the MFCC.\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.xlabel='Frame Number'\n",
    "plt.ylabel='MFCC Value'\n",
    "plt.xlim=(0, mfcc.shape[1])\n",
    "plt.ylim= (0, n_mfcc-1)\n",
    "plt.colorbar()\n",
    "plt.imshow(mfcc, origin='lower', aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On 2: Manipulating MIDI Data\n",
    "\n",
    "As discussed, we can also work with a more symbolic representation of our\n",
    "music data through the widespread MIDI format.\n",
    "\n",
    "<img src=\"./assets/midi_data.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and playing MIDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "# We will load a MIDI file using mido and examine the number of tracks and the\n",
    "# number of messages in each track.\n",
    "\n",
    "midi_file = mido.MidiFile('assets/symphony40.mid')\n",
    "print(\"Number of tracks:\", len(midi_file.tracks))\n",
    "\n",
    "# Remember, MIDI messages include both notes on and notes off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "# Visualizing MIDI data is not as straightforward as audio data, but we can do\n",
    "# so through a piano roll. A piano roll is a 2D representation of MIDI data,\n",
    "# where the x-axis is time and the y-axis is pitch. To create this\n",
    "# representation, we first need to define convert our event-based MIDI data\n",
    "# into a time-based representation.\n",
    "\n",
    "# Although there are some (limited) libraries that can convert MIDI to piano\n",
    "# rolls, we will use numpy to process our MIDI data and create an array that\n",
    "# represents a piano roll that we can visualize. Our piano roll will be of shape\n",
    "# (instruments, 128, max_ticks), where instruments is the number of instruments\n",
    "# in the MIDI file, 128 is the number of MIDI notes (0-127), and max_ticks is\n",
    "# the maximum number of ticks in the MIDI file.\n",
    "\n",
    "\n",
    "def midi_to_piano_roll(midi_file, ticks_per_beat=480):\n",
    "    ...\n",
    "    pass\n",
    "\n",
    "piano_roll = ...\n",
    "print(\"Piano roll shape:\", piano_roll.shape) # should be (instruments, 128, max_ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the piano roll using matplotlib. We will use the scatter\n",
    "# function for each instrument to plot the notes on the piano roll. The x-axis\n",
    "# will be the time in ticks and the y-axis will be the MIDI note number. The\n",
    "# color of each point will represent the instrument number.\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the first 20 000 ticks of the piano roll to get a better view.\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi2audio\n",
    "\n",
    "# Now, we can listen to the MIDI file using IPython's Audio class. We will use\n",
    "# the midi2audio library to synthesize the MIDI file into audio. We will use the\n",
    "# soundfont file \"assets/soundfont.sf2\" to synthesize the MIDI file. You can\n",
    "# download a soundfont file from the internet or use one of your own.\n",
    "\n",
    "soundfont_file = \"assets/soundfont.sf2\"\n",
    "\n",
    "midi2audio_obj = ...\n",
    "\n",
    "# Now we can load the synthesized audio file and listen to it.\n",
    "y, sr = ...\n",
    "print(\"Loaded MIDI audio with sample rate:\", sr)\n",
    "print(\"Audio shape:\", y.shape)\n",
    "print(\"Audio duration (seconds):\", y.shape[0] / sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
